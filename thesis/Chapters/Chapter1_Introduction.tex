% Chapter Template

\chapter{Introduction} % Main chapter title

\label{Chapter1} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

For a long time, the brain has been the subject of scientific research and has inspired other fields and technologies, including the domain of computer science. In the last decade, the technologies derived from it have grown popular and become a part of everyday life in the form of e.g. artificial neural networks and machine learning. And deeper insights into the brain's functioning have enabled medicine to treat patients more successfully. But despite all this progress, most of its exact inner workings including learning and memory are still largely unknown, although there is a consensus that the plasticity of synapses is the substrate that enables it.\\
For several decades now, the possibilities of gaining insights into this study object through the emerging computational neuroscience have been growing. Computational neuroscience can test, enhance and develop new models that describe biological reality, as well as help neuroscience by providing means of cerebral simulation.\\
One example of this is the study on sequence learning currently conducted by the department of Computational and Systems Neuroscience (INM-6) at the FZ JÃ¼lich, which also offered a small part of this work for a group of bachelor theses on different computational models of sequential learning.
This thesis is one of them and is aimed at replicating the learning model from \parencite{klampfl_maass_2013}.
\\ \ \\
In their work, the authors of \parencite{klampfl_maass_2013} modeled so-called pyramidal cells (PCs) - a type of neuron that appears for example in the prefrontal cortex and is assumed to play an important role in cognition \parencite{10.1093/cercor/bhg093} - with added \textit{lateral inhibition} by creating a modified version of liquid computing model. A liquid computing model (also called liquid state machine) implements a spiking neural network by using recurrent connections between nodes (neurons) that transmit temporal signals (spikes) that activate the nodes in a certain way. The name "liquid state machine" comes from the fact that each node has a continuous state instead of a discrete one, and can pass it on to neighboring nodes, similar to waves  \parencite{maass_markram_2004}. Although one might assume so due to the biological context, it has nothing to do with the plasticity (or liquidity) of connections between nodes.
The phenomenon of lateral inhibition which was also mentioned stems from neuroscience and causes a neuron to prevent its neighboring neurons from firing when it is itself active. In biological reality, this is usually realized by inhibitory synapses connected to the neighboring neurons. These PCs in combination with lateral inhibition tend to form so-called Winner-Take-All (WTA) cortical microcircuits.\\
The main goal of the said paper was to demonstrate how memory, learning, and even certain computational abilities can emerge from WTA cell assemblies with spike-timing-dependent plasticity (STDP; explained in \ref{ssec:spike_timing_dependent_plasticity} and \ref{ssec:stdp}) when presented with sequential input patterns. More specifically they postulated three constraints for this behavior \parencite{klampfl_maass_2013}:
\begin{enumerate}
    \item PCs and inhibitory neurons tend to be organized into specific network motifs
    \item synapses between PCs are subject to STDP
    \item neural responses are highly variable (\glqq trial-to-trial variability \grqq)
\end{enumerate}
By replicating the model from this paper its correctness is supposed to be verified while also providing an Open Source version of it that is accessible for further research and modification. In addition, any hidden assumptions made in the description of the original should be uncovered.\\
To do so, after explaining the methods of this work (Section \ref{sec:methods}), this thesis will first briefly discuss the biological backgrounds needed to help understand its content from the perspective of a Computer Scientist in Chapter \ref{Chapter2}. Following up on this in Chapter \ref{Chapter3} the original model of \parencite{klampfl_maass_2013} will be discussed in more detail and the exact model definitions will be laid out and put to inspection. This leads over to the summary of the implementation produced for this thesis (Chapter \ref{Chapter4}), building on the original implementation and pointing out justified alterations and different technical approaches. Finally, the results of both implementations will be compared in Chapter \ref{Chapter5}, followed by a conclusion and an outlook to future work (Chapter \ref{Chapter6}).
% TODO: explain 
%  sequence learning


\section{Methods} \label{sec:methods}
As already mentioned, this replication study should be viewed as part of a group of studies conducted as part of bachelor theses at the INM-6 (Institute for Medicine and Neuroscience, Computation in Neural Circuits group). All these studies have in common that they are implemented using NEST \parencite{nest_3_3} as their simulation framework to be able to compare the studied models in a more simulator-agnostic way.\\
Kindly the authors of \parencite{klampfl_maass_2013} agreed to share the Python 2.7 implementation code of their work, so when the implemented model deviates from the one in the paper, the implemented model is considered here, since it was used to produce the advertised results.\\
To study and understand the original model more closely, the first phase of this thesis was dedicated to replicating the results of the paper using their original code. The learnings of this phase are presented in Chapter \ref{Chapter3}. Along with replicating the results during this phase, it was also decided to add several means of recording and visualizing certain parameters like membrane potentials, spike activity, and synaptic weights, in order to be able to compare the results with those of the later replication of the model to verify its correctness.\\
After getting familiar with the original implementation, the replication phase started. For the model replication NEST 3.3 \parencite{nest_3_3} was initially chosen but later replaced with the most recent development version at the time. 
NEST (\textbf{NE}ural \textbf{S}imulation \textbf{T}ool) is a simulator for spiking neural networks developed by the NEST initiative and is built to simulate various forms of learning and plasticity. While NEST itself is a standalone application operating on a C++ kernel, it is also possible to run it in Python using PyNEST. This program can translate Python commands for NEST. Due to the superior simplicity and readability of Python compared to C++, the PyNEST option was selected for this thesis.\\
As further elaborated later, entirely new neuron and synapse models are needed, which are not provided by NEST natively. To create these models for NEST, the modeling language NESTML \parencite{nestml_5_0_0} was chosen. NESTML creates neuron and synapse models by translating instructions formulated in the NESTML modeling languages syntax to C++ code that integrates with the main NEST kernel. The language was created to make the process of writing custom neuron and synapse models easier and most importantly more accessible since the construction of such models by hand requires a vast knowledge of the NEST kernel in addition to proficiency in C++ programming. Without NESTML this would further restrict users from working with NEST.\\
During the work on implementing the synapse and neuron models from \parencite{klampfl_maass_2013}, it eventually became clear that NESTML was not capable of fulfilling the requirements of the model. For this reason, NESTML could not further be used to create the models. The incomplete generated model implementations of NESTML were however used as the platform to build a highly customized C++ model by hand (described in Section \ref{sec:neuron_synapse_models}).\\
For the Python implementation of the network NumPy and matplotlib were used to respectively help with data handling and plotting of readout data. 
% TODO continue with explanation of comparison etc
